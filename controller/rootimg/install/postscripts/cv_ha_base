#-----------------------------------------------------------------------------------------------
# first we setup pacemaker
#-----------------------------------------------------------------------------------------------

yum -y install epel-release
yum -y install drbdlinks
yum -y install pacemaker
yum -y install pcs fence-agents-all
systemctl start pcsd
systemctl start pacemaker 

echo system | passwd hacluster --stdin

yum -y install http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm
yum -y install drbd84-utils kmod-drbd84

#-------------------------------------------------------------
# configure drbd
#------------------------------------------------------------

# remove the drbd volume from mounting
sed -i -e "/vg_root-lv_drbd/d" /etc/fstab
umount /drbd

# Just to make sure, we clear the disk
echo Clearing drbd logical volume, this may take some time.
dd if=/dev/zero of=/dev/mapper/vg_root-lv_drbd bs=1M count=1024
echo Clearing drbd logical volume, done.

ip1=10.141.255.253
ip2=10.141.255.252

cat <<EOF >/etc/drbd.d/ha_disk.res
resource ha_disk {
       net {
         after-sb-0pri discard-least-changes;
         after-sb-1pri consensus;
         after-sb-2pri call-pri-lost-after-sb;
       }
       on controller-1.cluster {
         device    /dev/drbd1;
         disk      /dev/mapper/vg_root-lv_drbd;
         address   ${ip1}:7789;
         meta-disk internal;
       }
       on controller-2.cluster {
         device    /dev/drbd1;
         disk      /dev/mapper/vg_root-lv_drbd;
         address   ${ip2}:7789;
         meta-disk internal;
       }
     }
EOF

drbdadm create-md ha_disk
modprobe drbd
drbdadm up ha_disk

if [ $NODE = 'controller-1' ]; then 
   # Here we should setup some type of waiting mechanism until controller-2 is ready to be configured.
   for i in {1..100}; do
       if ssh controller-2 cat /tmp/cv_ha_ready; then break; fi
       sleep 5
   done
      
   pcs cluster auth controller-1.cluster controller-2.cluster -u hacluster -p system
   pcs cluster setup --name trinity controller-1.cluster controller-2.cluster --force
   pcs cluster start --all
else
   touch /tmp/cv_ha_ready
fi

echo "$0 finished"
echo "$0 finished" >> /var/log.postinstall.log
