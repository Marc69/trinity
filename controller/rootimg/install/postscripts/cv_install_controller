#! /usr/bin/bash
##description    : Setup the first round of High Availability.
##	           Basic pacemaker and xCAT configuration required on both nodes.
##author         : Hans Then 
##email          : hans.then@clustervision

#-----------------------------------------------------------------------------------------------
# Determine if this will be the active or passive node. An odd node will become 
# the active node. Any even node will become the passive node. 
#
# Adapt the network configuration accordingly.
#-----------------------------------------------------------------------------------------------
n=$(echo $NODE | egrep -o '[[:digit:]]*' | head -n1)
# normalize to 1 or 2
N=$((((n-1)%2)+1))
# set the new ip addresses
octet=$((254-$N))

set -x
echo system | passwd hacluster --stdin
echo drbd > /etc/modules-load.d/drbd.conf

#------------------------------------------------------------
# configure drbd
#------------------------------------------------------------
# remove the drbd volume from mounting
sed -i -e "/vg_root-lv_drbd/d" /etc/fstab
umount /drbd

# Clear the disk
echo Clearing drbd logical volume, this may take some time.
dd if=/dev/zero of=/dev/mapper/vg_root-lv_drbd bs=1M count=1024

# make sure the host names can be resolved
if ! grep 10.141.255.253 /etc/hosts; then echo "10.141.255.253 controller-1 controller-1.cluster" >> /etc/hosts; fi
if ! grep 10.141.255.252 /etc/hosts; then echo "10.141.255.252 controller-2 controller-2.cluster" >> /etc/hosts; fi
if ! grep 10.141.255.254 /etc/hosts; then echo "10.141.255.254 controller controller.cluster" >> /etc/hosts; fi
hostnamectl set-hostname controller-${N}.cluster

# If we can use the IB interface for DRBD we will do so.
if ping -c1 10.149.255.$octet > /dev/null 2>&1; then
    ip1=10.149.255.253
    ip2=10.149.255.252
else
    ip1=10.141.255.253
    ip2=10.141.255.252
fi

cat <<EOF >/etc/drbd.d/ha_disk.res
resource ha_disk {
       net {
         after-sb-0pri discard-younger-primary;
         after-sb-1pri discard-secondary;
         after-sb-2pri disconnect;
       }
       on controller-1.cluster {
         device    /dev/drbd1;
         disk      /dev/mapper/vg_root-lv_drbd;
         address   ${ip1}:7789;
         meta-disk internal;
       }
       on controller-2.cluster {
         device    /dev/drbd1;
         disk      /dev/mapper/vg_root-lv_drbd;
         address   ${ip2}:7789;
         meta-disk internal;
       }
}
EOF

drbdadm create-md ha_disk
modprobe drbd
drbdadm up ha_disk

#-----------------------------------------------------------------------------------------------
# Install pacemaker
#-----------------------------------------------------------------------------------------------
systemctl start pcsd
systemctl enable pcsd

pcs cluster auth $(hostname) -u hacluster -p system
if [[ $N = 1 ]]; then
    pcs cluster setup --name ha_cluster $(hostname)
else
    for i in {1..100}; do
        ssh controller-1 pcs status && break
        echo "waiting for master to come up"
        sleep 1m
    done
    [[ $i = 100 ]] && echo "TIMEOUT: waiting for master node to come up"
    ssh controller-1.cluster pcs cluster auth $(hostname) -u hacluster -p system
    ssh controller-1.cluster pcs cluster node add $(hostname)
    pcs cluster auth
    scp -r controller-1.cluster/root/.xcat /root
fi
pcs cluster start

systemctl enable pacemaker
systemctl enable corosync
systemctl enable opensm
systemctl start opensm
systemctl enable ntpd
systemctl start ntpd

#----------------------------------------------------------------------------------
# xCAT setup required on both nodes
#----------------------------------------------------------------------------------
systemctl stop NetworkManager
systemctl disable NetworkManager
killall dhclient

# Fix for #356
systemctl disable firewalld
systemctl stop firewalld

#read ETH1 ETH2 ETH3 <<<$(ls /sys/class/net/ | grep "^e" | sort | head -3)
ETH1=$(ip route get 8.8.8.8 | grep -oP 'dev \K\S+')
ETH2=$(ip route get 10.141.0.0 | grep -oP 'dev \K\S+')
# setup NAT, so nodes can access the internet (see manual step 1.f)
systemctl enable iptables 
systemctl start iptables
modprobe iptable_nat

iptables -A FORWARD -i $ETH2 -j ACCEPT
iptables -t nat -A POSTROUTING -o $ETH1 -j MASQUERADE
iptables -D FORWARD -j REJECT --reject-with icmp-host-prohibited
iptables -D INPUT -j REJECT --reject-with icmp-host-prohibited
service iptables save

#--------------------------------------------------------------------------------------
# setup routing for the virtual clusters
#--------------------------------------------------------------------------------------
ip route add 172.16.0.0/12 dev $ETH2
echo "172.16.0.0/12 dev $ETH2" > /etc/sysconfig/network-scripts/route-${ETH2} 

if ! systemctl restart network; then
    sleep 3
    systemctl start network
fi

setenforce 0
sestatus

#-----------------------------------------------------------------------------------------
# Patch xCAT. Needs to be done on both nodes.
#-----------------------------------------------------------------------------------------
if [ ! -z $SITEMASTER ]; then
    mkdir /tmp/trinity
    mount ${SITEMASTER}:/trinity /tmp/trinity       
fi

cd /opt/xcat/share/xcat/netboot/centos
ln -s ../rh/dracut_033 .
ln -s ../rh/dracut .
cd /
cat /tmp/trinity/controller/xcat/patches/*.patch | patch -p0
systemctl restart xcatd

#--------------------------------------------------------------------------------------
# copy required files from the master node to the controller node
#--------------------------------------------------------------------------------------
cp -LrT /tmp/trinity/controller/rootimg /
cp /tmp/trinity/version /trinity/version
if [[ -f /tmp/trinity/site ]]; then
    cp /tmp/trinity/site /trinity/site
fi
mkdir -p /var/lib/docker-registry

#------------------------------------------------------------------------------
# Install and configure Horizon
#------------------------------------------------------------------------------
cp -LrT /trinity/horizon/rootimg/ /
chown -R apache:apache /usr/share/openstack-dashboard/static
if ! grep conf.modules.d/ /etc/httpd/conf/httpd.conf; then
    echo 'Include "/etc/httpd/conf.modules.d/*.conf"' >> /etc/httpd/conf/httpd.conf
fi

#-----------------------------------------------------------------
# Install the API server, client and dashboard additions
#-----------------------------------------------------------------
cp -LrT /trinity/openstack/rootimg/ /

#-- Trinity API setup
cd /opt/trinity/trinity_api
python setup.py install
cd -

#Install the python client
cd /opt/trinity/trinity_client
python setup.py install
cd -

#-- Install the dashboard additions
cd /opt/trinity/trinity_dashboard
sh setup.sh
cd -

set -x
# FIXME: use obol for this.
useradd -u 1002 -U munge --no-create-home

mkdir -p /var/log/trinity
# FIXME: rather hackish, ensure this directory exists on both nodes.
mkdir -p /nfshome
systemctl daemon-reload

#---------------------------------------------------------------------------------------
# Set up the xCAT web service
#---------------------------------------------------------------------------------------
source /etc/profile.d/xcat.sh
export HOME=/root
# FIXME: use obol for this.
useradd trinity
echo system | passwd trinity --stdin
mkdef -t policy 6 name=trinity rule=allow
sed  -e "/<Directory \/>/,/<\/Directory>/ s/Require/#Require/" -i /etc/httpd/conf/httpd.conf
systemctl restart httpd 

#Make xCAT available via the floating IP.
if grep -v XCATHOST /etc/profile.d/xcat.sh; then
    echo "export XCATHOST=controller.cluster:3001" >> /etc/profile.d/xcat.sh
fi

systemctl start replicator.timer

echo "$0 finished @ $(date)" >> /var/log/postinstall.log
